{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPyImage\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Adaptive thresholding handles lighting variations\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, 11\n",
    "    )\n",
    "    return image, thresh\n",
    "\n",
    "def run_tesseract_ocr(thresh_img, fx=1, fy=1):\n",
    "    data = pytesseract.image_to_data(thresh_img, output_type=pytesseract.Output.DICT)  \n",
    "    results = []\n",
    "    for i in range(len(data['text'])):\n",
    "        text = data['text'][i].strip()\n",
    "        conf = int(data['conf'][i])\n",
    "        x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
    "        bbox = [[x, y], [x + w, y], [x + w, y + h], [x, y + h]]\n",
    "        results.append((bbox, text, conf))\n",
    "    return results, fx, fy  # Return scaling factors too\n",
    "\n",
    "def draw_group_annotations(image_path, groups, fx=1.0, fy=1.0, output_path=\"group_annotated.png\"):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    for idx, group in enumerate(groups, 1):\n",
    "        # Collect all points from group and scale them back\n",
    "        all_pts = []\n",
    "        for bbox, _, _ in group:\n",
    "            scaled_bbox = [[int(x / fx), int(y / fy)] for x, y in bbox]\n",
    "            all_pts.extend(scaled_bbox)\n",
    "\n",
    "        # Compute bounding box for group\n",
    "        all_pts = np.array(all_pts)\n",
    "        x_min, y_min = all_pts[:, 0].min(), all_pts[:, 1].min()\n",
    "        x_max, y_max = all_pts[:, 0].max(), all_pts[:, 1].max()\n",
    "        height = y_max - y_min\n",
    "\n",
    "        # Draw merged box\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "\n",
    "        # Combine text\n",
    "        merged_text = \" \".join([text for _, text, _ in group])\n",
    "        cv2.putText(image, merged_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        print(f\"[Group {idx}] {merged_text} : height={height}\")\n",
    "\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"‚úÖ Group-annotated image saved: {output_path}\")\n",
    "    display(Image.open(output_path))\n",
    "\n",
    "def group_adjacent_by_largest(ocr_results, tolerance=10, distance_threshold=50):\n",
    "    blocks = []\n",
    "    for bbox, text, _ in ocr_results:\n",
    "        y_top = bbox[0][1]\n",
    "        y_bottom = bbox[2][1]\n",
    "        height = abs(y_bottom - y_top)\n",
    "        x_center = int((bbox[0][0] + bbox[2][0]) / 2)\n",
    "        y_center = int((bbox[0][1] + bbox[2][1]) / 2)\n",
    "        blocks.append({\n",
    "            \"text\": text,\n",
    "            \"bbox\": [[int(x), int(y)] for x, y in bbox],\n",
    "            \"height\": height,\n",
    "            \"center\": (x_center, y_center)\n",
    "        })\n",
    "\n",
    "    if not blocks:\n",
    "        return []\n",
    "\n",
    "    # Step 1: Find the block with the highest height\n",
    "    largest_block = max(blocks, key=lambda b: b[\"height\"])\n",
    "    max_height = largest_block[\"height\"]\n",
    "\n",
    "    # Step 2: Adjust tolerance if needed\n",
    "    if \",\" in largest_block[\"text\"]:\n",
    "        tolerance = max_height - 50\n",
    "\n",
    "    print(f\"üìè Largest block: {largest_block['text']} (height: {max_height})\")\n",
    "    print(f\"üéØ Using tolerance: {tolerance}\")\n",
    "\n",
    "    # Step 3: Find blocks close in height and distance to the largest\n",
    "    group = [largest_block]\n",
    "    largest_center = largest_block[\"center\"]\n",
    "\n",
    "    for block in blocks:\n",
    "        if block == largest_block:\n",
    "            continue\n",
    "\n",
    "        height_diff = abs(block[\"height\"] - max_height)\n",
    "        if height_diff <= tolerance:\n",
    "            # dx = block[\"center\"][0] - largest_center[0]\n",
    "            # dy = block[\"center\"][1] - largest_center[1]\n",
    "            # dist = (dx**2 + dy**2)**0.5\n",
    "\n",
    "            # if dist <= distance_threshold:\n",
    "            group.append(block)\n",
    "\n",
    "    # Step 4: Return grouped blocks in (bbox, text, conf) format\n",
    "    grouped_results = []\n",
    "    for block in group:\n",
    "        # Re-find original confidence from input results\n",
    "        for bbox, text, conf in ocr_results:\n",
    "            if text == block[\"text\"] and block[\"bbox\"] == [[int(x), int(y)] for x, y in bbox]:\n",
    "                grouped_results.append((bbox, text, conf))\n",
    "                break\n",
    "\n",
    "    return [grouped_results]\n",
    "\n",
    "\n",
    "def draw_ocr_boxes(image_path, ocr_results, fx=1.0, fy=1.0, output_path=\"ocr_annotated.png\"):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    for idx, (bbox, text, conf) in enumerate(ocr_results, 1):\n",
    "        scaled_bbox = [[int(x / fx), int(y / fy)] for x, y in bbox]\n",
    "        x_min, y_min = scaled_bbox[0]\n",
    "        x_max, y_max = scaled_bbox[2]\n",
    "\n",
    "        # Draw box\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        # Draw text\n",
    "        cv2.putText(image, text, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        print(f\"[{idx}] {text} (conf: {conf})\")\n",
    "\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"‚úÖ OCR annotated image saved to {output_path}\")\n",
    "    display(Image.open(output_path))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def clean_and_structure_ocr_results(raw_ocr_results):\n",
    "    \"\"\"\n",
    "    Cleans and structures raw OCR results into a list of dictionaries\n",
    "    suitable for an NLP API prompt.\n",
    "\n",
    "    Args:\n",
    "        raw_ocr_results (list): The raw output from your OCR engine,\n",
    "                                 e.g., [([[x1,y1],...], 'text', confidence), ...]\n",
    "        confidence_threshold (int): Minimum confidence score for a text\n",
    "                                    entry to be included. Default is 70.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a clean text block\n",
    "              with its text, bbox (x, y, w, h), and estimated font_height.\n",
    "              Sorted by Y-coordinate then X-coordinate.\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "\n",
    "    for item in raw_ocr_results:\n",
    "        # Unpack the tuple\n",
    "        coords_list, text, confidence = item\n",
    "\n",
    "        # 1. Filter out empty text and low confidence scores\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        # 2. Extract and convert bounding box coordinates to (x, y, w, h)\n",
    "        # Your coordinates are [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]\n",
    "        # We need to find min/max x and y\n",
    "        x_coords = [p[0] for p in coords_list]\n",
    "        y_coords = [p[1] for p in coords_list]\n",
    "\n",
    "        min_x = min(x_coords)\n",
    "        max_x = max(x_coords)\n",
    "        min_y = min(y_coords)\n",
    "        max_y = max(y_coords)\n",
    "\n",
    "        bbox_x = min_x\n",
    "        bbox_y = min_y\n",
    "        bbox_width = max_x - min_x\n",
    "        bbox_height = max_y - min_y\n",
    "\n",
    "        # Handle cases where height or width might be zero or negative (invalid bbox)\n",
    "        if bbox_width <= 0 or bbox_height <= 0:\n",
    "            continue\n",
    "\n",
    "        # 3. Estimate font height (using bbox_height)\n",
    "        # Note: True font size is complex, but bbox_height is a good proxy.\n",
    "        estimated_font_height = bbox_height\n",
    "\n",
    "        # Add to cleaned data\n",
    "        cleaned_data.append({\n",
    "            \"text\": text.strip(),\n",
    "            \"bbox\": [bbox_x, bbox_y, bbox_width, bbox_height],\n",
    "            \"font_height\": estimated_font_height,\n",
    "            \"confidence\": confidence # Keep confidence if useful for further analysis\n",
    "        })\n",
    "\n",
    "    # 4. Sort the data: Crucial for understanding document flow\n",
    "    # Sort primarily by y-coordinate (top to bottom), then by x-coordinate (left to right)\n",
    "    cleaned_data.sort(key=lambda item: (item['bbox'][1], item['bbox'][0]))\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def draw_annotations_from_json(image_path, json_data, fx=1.0, fy=1.0, output_path=\"annotated_articles.png\"):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"‚ùå Could not load image from: {image_path}\")\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        articles = json.loads(json_data)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"‚ùå Invalid JSON data\")\n",
    "\n",
    "    # Define annotation types and colors\n",
    "    parts = {\n",
    "        \"headline\": ((255, 0, 0), 2),       # Blue\n",
    "        \"subheadline\": ((0, 255, 0), 2),    # Green\n",
    "    }\n",
    "\n",
    "    for article in articles.get(\"values\", []):\n",
    "        article_type = article.get(\"type\")\n",
    "        bbox = article.get(\"bbox\")  # [x, y, w, h]\n",
    "\n",
    "        if article_type in parts and bbox:\n",
    "            color, thickness = parts[article_type]\n",
    "            x, y, w, h = bbox\n",
    "\n",
    "            # Scale bbox back to original image size\n",
    "            x = int(x / fx)\n",
    "            y = int(y / fy)\n",
    "            w = int(w / fx)\n",
    "            h = int(h / fy)\n",
    "\n",
    "            top_left = (x, y)\n",
    "            bottom_right = (x + w, y + h)\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right, color, thickness)\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                article_type.capitalize(),\n",
    "                (x, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                color,\n",
    "                2\n",
    "            )\n",
    "\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"‚úÖ Annotated image saved to: {output_path}\")\n",
    "    Image.open(output_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Annotated image saved to: annotated_articles.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Run the pipeline\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "image_path = \"./Label Studio/imgs/page_1.png\"  # Replace with your image path\n",
    "\n",
    "original_img, thresholded = preprocess_image(image_path)\n",
    "ocr_results, fx, fy = run_tesseract_ocr(thresholded)\n",
    "cleaned_results = clean_and_structure_ocr_results(ocr_results)\n",
    "genai.configure(api_key=API_KEY) \n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "json_sample = {\n",
    "    \"values\" : [\n",
    "          {\n",
    "        \"type\" : \"headline\",\n",
    "        \"text\" : \"nLOUSe tO OMEN DicamM to pulDllic\",\n",
    "        \"bbox\" : [155, 1720, 3460, 313]\n",
    "        },\n",
    "        {\n",
    "        \"type\": \"subheadline\",\n",
    "        \"text\": \"Senate minority bloc backs more transparency to end corruption\",\n",
    "        \"bbox\": [205, 2075, 3334, 129]\n",
    "        }\n",
    "    \n",
    "    ]\n",
    "}\n",
    "processed_img = Image.open(image_path)\n",
    "prompt = [\n",
    "    \n",
    "]\n",
    "\n",
    "# Send a prompt\n",
    "response = model.generate_content(f\"\"\"Here is a list of text blocks/lines extracted from a newspaper page, along with their approximate bounding box dimensions (x, y, width, height) and estimated font heights. Please identify the main headline and any associated subheadlines for each distinct article. Group them logically. Prioritize lines with larger font heights and top-of-column positions as potential headlines.\n",
    "\n",
    "Data:\n",
    "{json.dumps(cleaned_results, indent=2)}\n",
    "\n",
    "Based on this data, please provide the identified main headlines and their subheadlines (if available), put the whole headline inside a big bounding box, do the same with subheadlines(if they exist). Please only send json file following the structure (without any text, just the json):\n",
    "{json.dumps(json_sample)}\n",
    "\"\"\")\n",
    "\n",
    "draw_annotations_from_json(image_path, response.text.strip(\"```json\"))\n",
    "# draw_ocr_boxes(image_path, ocr_results, fx=fx, fy=fy)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
